---
title: "logistic_regression_examples"
author: "Vincent Wauters"
date: "2018 M09 3"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Logistic Regression
## Logistic - Low Dimensional
### Logistic - LD - No Outliers

First we will generate data for an univariate example. Generally there are two approaches to creating data for logistic regression cases. Note that we are only focusing on the Bernoulli case and not on the Binomial case (i.e. $n_i = 1$ in usual formulas). The first approach is that of parametrizing the Bernoulli parameter $p_i$ (often: $p$) by the inverse logit-link. The second one is to assume a linear model and use a cut-off on it to generate binary data, this underlying model is called a latent model. The first method is more interesting because it allows us easier to calculate quantities which require the true $\boldsymbol{\beta}$.

```{r logistic_regression_dgp}
# Sample size
n <- 1000

# Beta
beta0 <- 0.2
beta1 <- 1.5

beta <- c(beta0, beta)

# Predictor X
X <- rnorm(n = n,
           mean = 0,
           sd = 1)

# Linear predictor eta
eta <- 1 + beta1 * X

# Bernoulli parameter 
pi <- exp(eta) / (1 + exp(eta))

# Generating Bernoulli outcome
y <- rbinom(n = n,
            size = 1,
            p = pi)

# Putting in data.frame
data <- data.frame("y" = y, "X" = X)
```

Because the data is univariate ($p = 1$) it can be plotted easily. Also to show the relation between the underlying probabilities and the outcomes, these are both plotted. Normally one does not know the true underlying probability ofcourse.

```{r plotting_ld_data}
plot(x = X,
     y = y,
     main = "Plot of DGP including underlying probabilities")
points(x = X,
      y = pi,
      pch = 5,
      col = "lightgray")
legend(x = "bottomright",
       legend = c("y-outcomes", "Underlying true pi"),
       pch = c(1, 5),
       col = c("black", "lightgrey"))
```

This also is a good time to show the impact of the signs and sizes of beta. We will now plot the logistic function $f(x) = exp(\beta_0 + \beta_1 * X)/(1 + exp(\beta_0 + beta_1 * x))$ and see what happens.

```{r}
# Setting beta
beta0 <- c(-3, 0, 3)
beta1 <- c(-10, -1, 0, 1, 10)

# Creating X-grid
X_grid <- seq(from = -3.5, to = 3.5, by = 0.01)

# Plotting
par(mfrow = c(3, 2)) # 6 frames
for(i in 1:length(beta1)){ # First a plot for each beta1
    plot(x = X_grid,
       y = (exp(beta0[1] + beta1[i] * X_grid)/(1 + exp(beta0[1] + beta1[i] * X_grid))),
       type = "l",
       ylim = c(0, 1),
       xlim = c(-3.5, 3.5),
       xlab = "X",
       ylab = "logistic(x)",
       main = paste0("Beta1 = ", beta1[i]))
  for(j in 2:length(beta0)){ # Adding lines for each beta0
    lines(x = X_grid,
          y = (exp(beta0[j] + beta1[i] * X_grid)/(1 + exp(beta0[j] + beta1[i] * X_grid))),
          col = j)
  }
}
```

Note that the red line is beta0 = 0, the black line is beta0 = -3 and the green line = beta0 = 3. The impact of $\beta_1$ is quite clear, if it's positive then the curve has an S-shape from low to high going from left to right. If it is negative it will have a mirrored (along the vertical axis) shape, i.e. starting high ending low from left to right. If it is exactly 0 then the line is flat, no matter the value of $\beta_0$. This case it the most interesting to see the impact of $\beta_0$. Remember that one can view $\beta_0$ as being related to the proportions in the sample when no covariate information is available. The plotted curves give the probability of the outcome being 1, i.e. Indeed if $\beta_0 = -3$ (black line) then this means that $P(y = 1) = exp(-3)/(1 + exp(-3))$ which is around 5%. If $\beta_0 = 0$ then the marginal distribution of $y$ is balanced hence the red line appearing at $p = 0.5$. Generally the impact of $\beta0$ is a parallel shift but it's a bit more complicated than that... (to do)

Through the use of the use of the linear predictor $\eta$ we can get a more unified view. 
```{r}
# Creating grid of all beta combinations
beta_grid <- expand.grid(beta0, beta1)
names(beta_grid) <- c("beta0", "beta1")
X_grid <- seq(-10, 10, by = 0.01)

# Creating eta (each column is the situation for dfferent beta combination)
eta <- matrix(NA, nrow = length(X_grid), ncol = nrow(beta_grid))
for(i in 1:nrow(beta_grid)){
  eta[, i] <- beta_grid[i, 1] + beta_grid[i, 2] * X_grid
}

# Probabilities corresponding to eta
probs <- exp(eta) / (1 + exp(eta))

# Plotting (using dots -> better for cases where beta1 = 0)
par(mfrow = c(2, 2))
for(i in 1:nrow(beta_grid)){
  plot(x = eta[, i],
       y = probs[, i],
       pch = 20,
       cex = 1.5,
       ylim = c(0, 1),
       main = paste0("Impact of eta for beta0 = ", beta_grid[i, 1], ", beta1 = ", beta_grid[i, 2]),
       xlab = "eta",
       ylab = "Probability")
}
par(mfrow = c(1, 1))
# Less clear than the overlaid plots from earlier (all beta0 values on 1 curve)
```

From the figures, we can see that all the curves start at 0 and go to 1 from left to right for increasing $\eta$. However in the cases where $\beta_1 = 0$ the linear predictor $\eta$ is a constant, hence it only produces a single point on the plot, all the outcome values are coinciding. For cases with a largue value of $\beta_0$ one also notices that the linear predictor only starts from a certain value, this is because if $\beta_0$ is this high (e.g. 3) it would assume $\exp(3) / (1 + exp(3))$ = 95\%$ probability when the $X$-values are 0, hence a very big __negative__ $X$ is needed to bring this down to 0 again. To achieve this the $X$-grid was made bigger.

__In any case the take-away message from these plots are: bigger higher probabilities of an 1-outcome. However we should see if this still makes sense in multiple dimensions. Since we can check the linear predictor (it's a scalar) and the probability (scalar as well) this is possible regardless the dimensionality of $X$.__

To do: linear predictor thinking in multiple dimensions.

__Fitting__ a logistic regression model by maximum likelihood estimation (MLE) can be done easily with the function `glm(..., family = "binomial")` a robust version is offered by the `ltsReg()` function (capital R!). For robust fits, the `glmrob(..., family = "binomial")` (small r!) function from the package `robustbase` (same package as `ltsReg()`) can be used.

There are multiple methods, inter alia the Bianco-Yohai (BY) method (`method = "BY"`) and the weighted Bianco-Yohai method (). However it is very important to remember that these methods are not based on the trimming idea but probably do belong in the M-estimator framework. The comparison of these non-trimming estimators with the MLE might not be very useful. __Actually only the __Bianco-Yohai robustified deviances are used in the elastic net-LTS estimator. This is necessary to pick the top performing model among different candidate models.__ (needs more info) 

```{r logistic_ld_fitting}
# Logit MLE
logit_mle1 <- glm(y ~ X, family = "binomial", data = data)
summary(logit_mle1)

# Logit BY 
logit_by1 <- glmrob(y ~ X, family = "binomial", method = "BY", data = data)
summary(logit_by1)

# Logit WBY
logit_wby1 <- glmrob(y ~ X, family = "binomial", method = "WBY", data = data)
summary(logit_wby1)
```

To get an idea of the fit one can plot the fitted probabilities on a fine grid of X-values. For `glm()` the `predict()` function works well, for `glmrob` objects there seems to be an error when requesting `type = response`.

```{r logistic_ld_predictions}
## Getting predictions
# Creating grid
X_grid <- data.frame("X" = seq(from = -3.5, to = 3.5, by = 0.001))

# MLE predictions
logit_mle1_preds <- predict(logit_mle1, newdata = X_grid, type = "response")

# BY predictions
logit_by1_preds <- 1/(1 + exp(-predict(logit_by1, newdata = X_grid)))

# WBY predictions
logit_wby1_preds <- 1/(1 + exp(-predict(logit_wby1, newdata = X_grid)))

## Plotting
plot(x = X,
     y = y,
     main = "Plotting fitted models on clean data")
# True underlying proabilities
points(x = X,
      y = pi,
      pch = 5,
      col = "lightgray")
# MLE
lines(x = X_grid$X,
      y = logit_mle1_preds)
# BY
lines(x = X_grid$X,
      y = logit_by1_preds,
      lty = 2)
# WBY
lines(x = X_grid$X,
      y = logit_wby1_preds,
      lty = 3)
# Legend
legend(x = "bottomright",
       legend = c("MLE", "BY", "WBY"),
       lty = 1:3)
legend(x = "topleft",
       legend = c("True p"),
       pch = 5,
       col = "lightgray")
```

What kinds of loss can we calculate? The simplest example is the negative loglikelihood loss. Another option is the MSE on $\pi$, a third option is the MSE on $\boldsymbol{\beta}$. A last option is the missclassification error (0-1 loss). We will do this on the generated data only, not on some X-grid. For a `glm`-object one can easily use the `fitted.values` attribute. For the others the predict function needs to be used.
```{r}
## Getting fitted values
# MLE
logit_mle1_preds <- logit_mle1$fitted.values

# BY
logit_by1_preds <- 1/(1 + exp(-predict(logit_by1, newdata = data.frame(X))))

# WBY
logit_wby1_preds <- 1/(1 + exp(-predict(logit_wby1, newdata = data.frame(X))))

## Calculating losses
# NLL
logit_mle1_nll <- logit_loss(y_true = y,
                             prob_hat = logit_mle1_preds)$avg_loss
logit_mle1_nll

logit_by1_nll <- logit_loss(y_true = y,
                            prob_hat = logit_by1_preds)$avg_loss
logit_by1_nll

logit_wby1_nll <- logit_loss(y_true = y,
                             prob_hat = logit_wby1_preds)$avg_loss
logit_wby1_nll
```

The negative loglikelihood loss seems to be almost exactly the same for the MLE, BY and WBY estimators.

### Logistic - LD - With Outliers

The most extreme and straightforward idea of an outlier in the univariate case for logistic regression would be that the linear predictor heavily suggests a certain outcome but actually the opposite is true. To better see what is meant, go back to the examples for different values of the parameters. For example for a case of positive $\beta_1$, a large value would mean more probability of a $y = 1$-outcome. Hence if a big $X$-value would suddenly lead to the outcome of $y = 0$, then clearly this is an outlier. 

For the case where $\beta_1 < 0$, the opposite holds, here a big $X$-value would on average lead to a very high probability of $y = 1$, if it was 0, this could be e regarded an outlier. 


It seems quite annoying to explain outliers in all the different cases, even in the univariate case there are at least 4 situations: for $\beta > 0, \beta < 0$, and for $y = 0, y = 1$ But using the linear predictor $\eta$. Yes indeed it generally holds that if $\eta$ is big, a high probability of $y = 1$ would be expected, hence if for big $\eta$ and the true $y = 0$, this would be an outlier. Conversely, if $\eta$ is small, and the true $y = 1$, then this would be an outlier as well.

TO DO
There are two types of outliers each time, outliers where the response of the outliers is $y = 0$ but should be $y = 1$ and vice versa. Also points can be outlying in the sense sense that they are outside the range of usual $X$-values. In any case an outlier that is only outlying in the range of $X$-values but has the "correct" output, is actually not problematic at all. 

Let us construct an univariate example with a positive and negative beta and then see the impact in $X$-space and in $\eta$-space.

```{r}
# Preparation
beta0 <- 0 # Intercept
X_grid <- seq(-3, 3, by = 0.001) # Plotting grid
par(mfrow = c(2, 2))

#### X-space
### Case: Beta1 positive
beta1 <- 1.5
p <- exp(beta0 + beta1 * X_grid)/(1 + exp(beta0 + beta1 * X_grid))
outliers_0 <- cbind(rnorm(n = 50,
                        mean = 2,
                        sd = 0.1),
                  rep(0, 50))
outliers_1 <- cbind(rnorm(n = 50,
                          mean = -2,
                          sd = 0.1),
                    rep(1, 50))

## Vertical outliers
plot(x = X_grid,
     y = p,
     type = "l",
     ylim = c(0, 1),
     main = "Vertical Outliers: Pos. Beta1 (X-space)",
     xlab = "X")
abline(h = 0, lty = 2)
abline(h = 1, lty = 2)
points(outliers_0,
       col = "red")
points(outliers_1,
       col = "blue")

## Horizontal outliers
outliers_hor0 <- cbind(rnorm(n = 50,
                       mean = -3.5,
                       sd = 0.1),
                       rep(0, 50))
outliers_hor1 <- cbind(rnorm(n = 50,
                       mean = 3.5,
                       sd = 0.1),
                       rep(1, 50))

plot(x = X_grid,
     y = p,
     type = "l",
     ylim = c(0, 1),
     xlim = c(-4, 4),
     main = "Horizontal Outliers: Pos. Beta1 (X-space)",
     xlab = "X")
points(outliers_hor0, col = "red")
points(outliers_hor1, col = "blue")


### Case: Negative Beta
beta1 <- -1.5
p <- exp(beta0 + beta1 * X_grid)/(1 + exp(beta0 + beta1 * X_grid))

## Vertical outliers
# Making
outliers_0_neg <- cbind(rnorm(n = 50,
                        mean = -2,
                        sd = 0.1),
                  rep(0, 50))
outliers_1_neg <- cbind(rnorm(n = 50,
                          mean = 2,
                          sd = 0.1),
                    rep(1, 50))

# Plotting Vertical outliers
plot(x = X_grid,
     y = p,
     type = "l",
     ylim = c(0, 1),
     main = "Vertical Outliers: Neg. Beta1 (X-space)",
     xlab = "X")
abline(h = 0, lty = 2)
abline(h = 1, lty = 2)
points(outliers_0_neg,
       col = "red")
points(outliers_1_neg,
       col = "blue")

## Horizontal outliers
outliers_hor0 <- cbind(rnorm(n = 50,
                       mean = 3.5,
                       sd = 0.1),
                       rep(0, 50))
outliers_hor1 <- cbind(rnorm(n = 50,
                       mean = -3.5,
                       sd = 0.1),
                       rep(1, 50))
# Plotting horizontal outliers
plot(x = X_grid,
     y = p,
     type = "l",
     ylim = c(0, 1),
     xlim = c(-4, 4),
     main = "Horizontal Outliers: Neg. Beta1 (X-space)",
     xlab = "X")
points(outliers_hor0, col = "red")
points(outliers_hor1, col = "blue")

par(mfrow = c(1, 1))


#### eta-space
# Function to calculate eta
eta <- function(X){
  eta <- beta0 + beta1 * X
  return(eta)
}

# Checking 2 beta cases
beta1 <- 1.5
p <- exp(beta0 + beta1 * X_grid)/(1 + exp(beta0 + beta1 * X_grid))
eta_grid <- eta(X_grid)

beta1 <- - 1.5
p <- exp(beta0 + beta1 * X_grid)/(1 + exp(beta0 + beta1 * X_grid))
eta_grid <- eta(X_grid)

plot(x = eta_grid,
     y = p,
     type = "l")

## To show that the sign of beta doesn't matter!
beta1 <- - 1.5
p <- exp(beta0 + beta1 * X_grid)/(1 + exp(beta0 + beta1 * X_grid))
eta_grid <- eta(X_grid)

# Adding line
lines(x = eta_grid,
      y = p,
      lty = 2) # It coincides

# Reconverting the X-coordinates of outliers to eta-coordinates
beta1 <- 1.5
outliers_0_eta <- cbind(eta(outliers_0[, 1]), rep(0, 50))
outliers_1_eta <- cbind(eta(outliers_1[, 1]), rep(1, 50))

par(mfrow = c(1, 2))
plot(x = eta_grid,
     y = p,
     type = "l",
     main = "Vert. outliers in eta-space")
points(outliers_0_eta, col = "blue")

plot(x = eta_grid,
     y = p,
     type = "l",
     main = "Vert. outliers in eta-space")
points(outliers_1_eta, col = "red")
par(mfrow = c(1, 1))
```

I am still a bit confused on how it exactly works with me putting the beta1 at positive value just before creating the eta coordinates...

# IDEA:
why don't we compare the fitted probabilities with those from the DGP based on something like the MSE?