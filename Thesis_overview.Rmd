---
title: "Thesis_Overview"
author: "Vincent Wauters"
date: "27 February 2018"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r installing_packages}
#install.packages("enetLTS")
#install.packages("robustbase")
#install.packages("mvtnorm")
#install.packages("MASS") # confint
#install.packages("robust")
#install.packages("MatrixModels")
#install.packages("glmnet")
```

```{r loading_libraries}
library(enetLTS)
library(robustbase)
library(mvtnorm)
library(MASS)
library(robust)
library(MatrixModels)
library(glmnet)
```

Sourcing functions defined in separate .R files.
```{r sourcing_functions}
source(file = "C://Users/Vincent Wauters/Google Drive/Schoolwerk/KU Leuven/Thesis_Sparse_Robust_Logistic_Regression/binary_reg_dgp1.R")
source(file = "C://Users/Vincent Wauters/Google Drive/Schoolwerk/KU Leuven/Thesis_Sparse_Robust_Logistic_Regression/logit_sim.R")
source(file = "C://Users/Vincent Wauters/Google Drive/Schoolwerk/KU Leuven/Thesis_Sparse_Robust_Logistic_Regression/logit_loss.R")
```

To get a better grasp of the concepts of robust and sparse modelling some simple examples will be given that isolate single issues and apply it on a simple models.

# Linear Models
## Linear - Low Dimensional (LD)
### Linear - LD - No Outliers

__USING OLS__

In a first step data is generated without outliers. A simple data generating process (DGP) $y = \beta_0 + \beta_1 * x_1 + \varepsilon$ and the same model is estimated by OLS.
```{r data_and_ols_no_outliers}
## Creating data
# Setting sample size
n <- 500

# Setting Coviates (Simple fixed design)
x1 <- c(rep(x = 10, times = floor(n/2)), rep(x = 20, times = floor(n/2)))

# Setting beta0, beta1
beta0 <- 100
beta1 <- 10

# Defining error
e <- rnorm(n = n,
            mean = 0,
            sd = 5)

# Defining the outcome
y <- beta0 + beta1 * x1 + e

## Fitting OLS
lm1 <- lm(y ~ x1) # (Includes intercept)
summary(lm1)

## Plotting
plot(x = x1,
     y = y,
     main = "No outliers, OLS fit")
abline(lm1)
```

__USING LTS__

A robust estimator for the linear model, the __Least Trimmed Squares (LTS)__ estimator can also be used, and should give the same results. The function `ltsReg()` (capital R) from the `robustbase` can be used. The default $\alpha = 0.5$ this leads to a $h$ sample size of $h = $ __to do__
```{r LTS_linear_no_outliers}
# Fitting LTS
lts1 <- ltsReg(y ~ x1, alpha = 0.5) # Default
summary(lts1)

# Fitting LTS with less trimming
lts2 <- ltsReg(y ~ x1, alpha = 0.95) # Closer to OLS
summary(lts2)

## Plotting LTS
plot(x = x1,
     y = y,
     main = "Clean data: OLS vs. LTS")
abline(lts1, col = "black") # LTS Line
abline(lm1, col = "gray", lwd = 2, lty = 2)
legend("bottomright",
       legend = c("OLS", "LTS"),
       lty = 1:2,
       lwd = 1:2,
       col = c("black", "gray"))
```

__CONCLUSION: Yes in the linear low dimensional case, LTS seem to do equally good (visually at least) as OLS in clean case__

__TO DO: what's the relation between alpha and the error variance (scale) estimate?__
__TO DO: I have no idea why the degrees of freedom on the residuals is not $n - p?!__


### Linear - LD - Outliers
Let's now introduce some __leverage points__, these are points that are outlying in the X direction. There are __good leverage points__ which have outlying X values but have a y-value that corresponds to the value that would be taken on by the relation follows from the other points. __Bad leverage points__ have outlying X values and have a non-fitting y value. 

Bad leverage points will be introduced, set at the coordinate (30, 150 + $e$) with $e$ coming from a $N(0, \sigma = 5)$ distribution or just put otherwise $y \sim N(150, 5)$. This is certainly a bad leverage point because a predicted reasonable value for $x1 = 30$ would be around 400.
```{r bad_leverage_point}
# Take look at current dataset
plot(x = x1,
     y = y,
     main = "Outliers Visualization in Linear Models")
abline(a = coef(lm1)[1],
       b = coef(lm1)[2])

# Introducing leverage points
n_outlier1 <- 50 # Amount of outliers
n2 <- n1 + n_outlier1

# Making them bad outliers
outliers1 <- rnorm(mean = 150, sd = 5, n = n_outlier1)

x1 <- c(x1, rep(30, n_outlier1))
y <- c(y, outliers1)

# Plotting
plot(x = x1,
     y = y,
     main = "OLS fit on regular data, bad leverage points added to display")
abline(lm1)
```

When fitting a model to this model that has 50/550 leverage points using OLS but also the LTS estimator:
```{r bad_leverage_ols_lts}
# Fitting OLS
lm2 <- lm(y ~ x1)
summary(lm2)

# Fitting LTS
lts2 <- ltsReg(y ~ x1)
summary(lts2)

# Plotting
plot(x = x1, 
     y = y,
     main = "Impact of bad leverage points")
abline(lm1, col = "gray", lwd = 2) # OLS on regular data only
abline(lm2, col = "black") # OLS on all data
abline(lts2, col = "red") # LTS on all dat
legend("bottomleft",
       legend = c("OLS no outliers",
                  "OLS outliers",
                  "LTS outliers"),
       lty = 1,
       col = c("gray", "black", "red"))
# OLS on the non-outlying points versus LTS: same, good!

```

Let's take a quick look at the LTS regression results on this dirty data:
```{r bad_leverage_lts_detail}
summary(lts2)
coef(lts2)

# Real betas:
beta0; beta1

```

The LTS recovers the true parameter values.

## Linear - High Dimensional
### Linear - HD - No Outliers

Let's now make something that is a lot more high dimensional, something with p = 200 (with intercept: 201), but only 10 first betas are nonzero, and the sample size stays the same at 500, __this example is thus NOT n < p!__

```{r}
## Setting parameters
n1 # Keepign same sample size = 500
p_a <- 10 # True dimensionality
p_b <- 190 # Uninformative
p <- p_a + p_b # Total dimensionality

## Creating data
beta0 <- 1
beta <- c(beta0, rep(1, p_a), rep(0, p_b))

# Creating covariates
X <- rmvnorm(n = n1,
             mean = rep(0, p),
             sigma = diag(p))
# Adding intercept column
X <- cbind(rep(1, times = n1), X)

# Adding names
colnames(X) <- paste0("X", 0:p)
  
# Creating outcome
y <- X %*% beta + rmvnorm(n = n1,
                          mean = 0)

# Creating dataframe used for FITTING
data1 <- data.frame(cbind(y, X[, -1]))
names(data1)[1] <- "y" # Setting name of y (first col)

## Fitting
# Making formula
formula1 <- paste("y", paste0("X", 1:p, collapse = " + "), sep = " ~ ")

# OLS
lm3 <- lm(formula = formula1, data = data1)
summary(lm3)

# LTS
lts3 <- ltsReg(formula = formula1, data = data1)
```
__ TO DO: further check the lts3 problems__

# Logistic Regression
## Low Dimensional - No Outliers

If we want multiple runs of such model (i.e. simulation), we need to run it multiple times, now we will automate this using a very simple function (which we will not use much further on, I suppose) and then using `replicate()` on this function. The function will generate data and specify the DGP and fit a logit model, then get the coefficients. By replicating this many times, due to the slightly different data (X) every time being sampled __and__ also due to sampling from the binomial distribution (i.e. even with the exact same probability parameter we ofcourse get different realizations).

The function is very basic, the parameters cannot even be altered in this simple version, it's purely as an example now!
```{r temporary_logitcoefs_simulation_function}
## Very Simple logit simulation: coefficients
logitcoefs <- function() {
  # Sample size
  N <- 1000
  
  # Beta
  beta0 <- 1
  beta1 <- 2
  beta2 <- 3
  
  # Linear predictor (eta)
  x1 <- rnorm(N)
  x2 <- rnorm(N)
  eta <- beta0 + beta1 * x1 + beta2 * x2
  
  # Probability (pi or p) and outcome (0/1)
  p <- exp(eta)/(1 + exp(eta))
  y <- rbinom(n = N, size = 1, prob = p)
  
  # Data.frame
  df <- data.frame(y = y, x1 = x1, x2 = x2)
  
  # Fitting
  fit <- glm(y ~ x1 + x2, data = df, family = binomial(link = "logit"))
  
  # Returning coefs
  return(coef(fit))
}

# Replicating and getting results:
results <- t(replicate(n = 50, 
                       expr = logitcoefs())) # Transposing for more easy to read results
colMeans(results) # Averages per variable
# Comparing to original beta = 1, 2, 3 -> YES It's OK

```


__to do: recheck these:__
Running simulations: first set up all the wanted settings, then looping over all these settings. To facilitate this we will create a settings matrix called `settings`.
```{r}
# Defining all simulation settings
beta_vector <- c(1, 2, 3) # True betas, same for all
n <- 500 # Sample size
runs <- 199
dirty <- 0
#types <- c("glm", "glmrob")
types <- "enetLTS"

# Putting in matrix
settings <- expand.grid(n = n, runs = runs, dirty = dirty, type = types)
settings
```

And each time gather all the settings by keeping the names the same every time and assesing row by row. Thus we are looping over the settings:
```{r simulation_loop_over_settings}
# Making model list of length = amount of rows of settings
model_list <- vector("list", nrow(settings))

# Looping over the settings:
for(i in 1:nrow(settings)){
  
  # To get an idea of progression
  print(paste("iteration", i))
  
  # Extracting current row of settings
  current_settings <- settings[i, ]
  
  # Saving all seperate settings in different variables
  n <- current_settings$n
  runs <- current_settings$runs
  dirty <- current_settings$dirty
  type <- current_settings$type

  # Running models
  model_list[[i]] <- logit_sim(beta_vector = beta_vector,
                               n = n,
                               runs = runs,
                               dirty = dirty,
                               type = type)
  
}
```

__to do: maybe add somekind of fields that contain the simulation settings__

Let us quickly define a simulation summary function that returns:

* The true coefficients, as set by the user
* The estimated means of the coefficients (mean over simulations)
* 
```{r sim_summary_function}
# Summary of simulation function:
sim_summary <- function(sim_object, beta_vector){
  coefs <- t(sapply(sim_object, coef))
  true <- beta_vector
  mean <- colMeans(coefs)
  bias <- colMeans(coefs - beta_vector)
  mse <- colMeans((coefs - beta_vector)^2)
  
  results <- rbind(true, mean, bias, mse)
  return(results)
}
```

And check the results
```{r}
lapply(model_list, sim_summary, beta_vector)
```


## Logistic - LD - Outliers
Let's try a very trivial __clean data__ setup, which does not actually allows me directly to see what the true $\beta$ is but gives some insights, far from ideal. We will take an increasing predictor $x_i = 1, 2, 3, ..., 100$ and an associated outcome $y_i = 0, ... 0, 1, ..., 1$, i.e. 0 for the first $2n/5$ (40%) and 1 for the last $2n/5$ (40%) observations, __for the middle we need some overlap in the observations to not have perfect separation__. To do this the middle $n/5$ (20%) variables are generated according to a Bernouilli distribution with $p = 0.5$.

```{r logit_manual_ld}
# Setting amount of obs:
n <- 200

# Predictors
X <- 1:n
  
# Outcome
set.seed(1234)
y <- c(rep(0, ((2/5) * n)), # 40 pct 0, 40pct 1, 20 pct: rbinom 50/50 in the middle to get OVERLAP
       rbinom(n = n/5, size = 1, prob = 0.50),
       rep(1, (2/5) * n))

# Data.frame:
data <- data.frame(y = y,
                   X = X)

# Plotting
plot(x = X,
     y = y,
     main = "Visualizing binary data")
```

Only clean data:
```{r plotting_logit}
# Fitting logit
logit1 <- glm(y ~ X, 
              family = binomial(link = "logit"),
              data = data)
logit1

# Getting predictions to plot:
logit1_preds_prob <- predict(logit1, type = "response")
logit1_preds_class <- ifelse(test = logit1_preds_prob > 0.5, yes = 1, no = 0)

# Plotting
plot(x = X,
     y = y,
     main = "Unidimensional Logit - No Outliers")
abline(h = 0.5, lty = 2)
lines(x = X,
      y = logit1_preds_prob)
points(x = X, 
       y = logit1_preds_class,
       pch = 2,
       col = "red")
legend("bottomright",
       legend = c("True Outcome",
                  "Class Prediction"),
       pch = 1:2,
       col = 1:2)
```

Now let's add leverage points to the example above somehow. We see that for X-values above (ca.) 100 the predictions are all 1, while those for X-values below 100 are 0. So let's try to add some 0-outcomes for those X-values above 200 (max of range of clean data), this will be very atypical, a leverage point. We will do this for the last 10 points of data (X = 190, ..., X = 200)
```{r plotting_logit_leverage}
# Setting y = 0 where it should be 1
data_dirty <- data # Copy
data_dirty[190:200, 1] <- 0 # Contaminate
data_dirty[190:200, 2] <- 290:300 # More leverage


# Fitting on dirty data
logit2 <- glm(y ~ X, 
              family = binomial(link = "logit"),
              data = data_dirty)
summary(logit2)

# Predictions
logit2_preds_prob <- predict(logit2, type = "response")
logit2_preds_class <- ifelse(test = logit2_preds_prob > 0.5, yes = 1, no = 0)

# Plotting
plot(x = data_dirty$X,
     y = data_dirty$y,
     main = "Unidimensional Logit - WITH Leverage Points",
     xlab = "X",
     ylab = "y")
lines(x = data_dirty$X,
      y = logit2_preds_prob)
points(x = X,
       y = logit2_preds_class,
       pch = 2,
       col = "red")
legend("topleft",
       legend = c("True Outcome",
                  "Class Prediction"),
       pch = 1:2,
       col = 1:2)

# Comparing coefs clean vs. dirty
coef(logit1)
coef(logit2)
```

Seeing how leverage points influence the fitted logit model and the related coefficients, the impact is obviously huge. However we actually not really know what the true value of the betas should be here due to our very simplistic setup!

Hence let's assume the following setup: we assume a Bernouilli distributed outcome, parametrized by the probability parmaeter $p$, and this $p = f(X, \beta)$, i.e. is some function of a structural component parametrized by $X$ and $\beta$. (See also: https://stats.stackexchange.com/questions/127226/logistic-regression-simulation-in-order-to-show-that-intercept-is-biased-when-y)

First, let us remind ourselves of the __logistic__ transformation (function), which is not the __logit__ transformation (it is the inverse logit). Let's present the logistic function in two equivalent forms:
$$f(x) = \frac{e^x}{1+e^x} = \frac{1}{1 + e^{-x}}$$

While the logit function is the inverse of this function and is in very general notation equal to:
$$f(x) = log(\frac{x}{1 - x})$$

Also note we can write the odds as the exponential taken of the log odds, as it is a 0-operation:

$$odds = \exp(log(odds))$$
Then see one can write the probability of the event $p$ (sometimes denotes as $\pi$) as (note we now start going from $x$ to $p$ to get a bit less general and more into the logit framework/language)

$$p = \frac{odds}{1 + odds}$$
To see why, remember that $odds = \frac{p}{1-p}$ so rewriting the above statement leads to the following, they should be equivalent (i.e. $p = p$) if the above statement is true

$$p = \frac{p/(1-p)}{1 + p/(1-p)} $$
$$p = \frac{p/(1-p)}{[(1-p)/(1-p)] + [p/(1-p)]} = \frac{p/(1-p)}{(1 - p + p)/(1-p)} = \frac{p/(1-p)}{1/(1-p)} = \frac{p (1-p)}{1 - p} = p$$
So indeed we have established that $p = \frac{odds}{1 + odds}$

Why is this useful? Because we also know that

$$log(odds) = \boldsymbol{X \beta}$$

In other words: the logit model is linear in terms of log-odds.

```{r logit_ld_alternative_dgp}
# Defining log-odds to probability function
lo2p = function(lo){
  odds = exp(lo)        
  p    = odds / (1 + odds)
  return(p)
}

# Setting DGP parameters
n <- 1000  # 1000 obs
beta0 <- 0.25 # intercept
beta1 <- 1 # slope
r <- 200 # amount of simulations

# Generating data
X <- rnorm(n) # Standard normal covariate
lo <- beta0 + beta1 * X # log-odds
p <- lo2p(lo) # Parameter vector of the Bernouilli
y <- rbinom(n = n, size = 1, prob = p) # 0/1 outcomes parametrized by p

# Also Ordering the data according to increasing X, makes life easier
idX <- order(X) # Indices, yes do this in 2 steps, otherwise messy!

X <- X[idX]
y <- y[idX]

# Running a single logit
logit3 <- glm(y ~ X,
              family = binomial(link = "logit"))

# Getting predictions to plot:
logit3_preds_prob <- predict(logit3, type = "response")
logit3_preds_class <- ifelse(test = logit3_preds_prob > 0.5, yes = 1, no = 0)



## Plotting
# True points
plot(x = X,
     y = y)
# Logit line
lines(x = X,
      y = logit3_preds_prob)

# Predicted points
points(x = X,
       y = logit3_preds_class,
       pch = 2,
       col = "red")
# Legend
legend("bottomright",
       legend = c("True Outcome",
                  "Class Prediction"),
       pch = 1:2,
       col = 1:2)

# These will store the estimation results
beta_hat0 <- numeric(r)  
beta_hat1 <- numeric(r)
y1_prop <- numeric(r) # Proportion of y = 1 in generated samples

# Running simulation (note: only y is random now, X fixed over samples)
for(i in 1:r){
  y <- rbinom(n = n, size = 1, prob = p)
  model <- glm(y ~ X, family = binomial(link = "logit"))
  beta_hat0[i] <- coef(model)[1]
  beta_hat1[i] <- coef(model)[2]
  y1_prop[i] <- mean(y)
}

## Results (over all simulation runs)
# Beta0
mean(beta_hat0)  
sd(beta_hat0)

# Beta1
mean(beta_hat1)
sd(beta_hat1)

# Proportion y = 1
mean(y1_prop) # Slightly unbalanced at beta0 = 0.25, beta1 = 1
```

Let's add some outliers now to the alternate DGP we had before. Therefore we take the last e.g. 50 points of the ordered X-values, make them a tad bigger (e.g. 5) and make the corresponding y-values 0 instead of 1.
```{r logit_alternative_contamination}
# Single logit
X[950:1000] <- 5 + rnorm(n = 1, mean = 0, sd = 1)
y[950:1000] <- 0

logit4 <- glm(y ~ X,
              family = binomial(link = "logit"))
coef(logit4) ; coef(logit3) # The contaminated one is completely off

## Plotting
# True points
plot(x = X,
     y = y,
     xlim = c(-3, 5.1))
# Logit line
lines(x = X,
      y = logit3_preds_prob)

# Predicted points
points(x = X,
       y = logit3_preds_class,
       pch = 2,
       col = "red")
# Legend
legend("bottomright",
       legend = c("True Outcome",
                  "Class Prediction"),
       pch = 1:2,
       col = 1:2)

```



# Logit - enetLTS

Now a single run of the robust and sparse estimator implemented in `enetLTS()` will be tried out. From the help file there are the following arguments:

* `xx`: a numeric matrix containing the predictor variables
* `yy`: response variable. For family = "binomial", this should be a factor coded as 0/1.
* `family`: a description of the error distribution and link function, "binomial" in our case
* `alphas`: ...
* ... 

[TO DO]
```{r enetLTS_data}
# Setting parameters    
n <- 1000
dirty <- 0.03
p <- 10
beta_vector <- c(1, 2, 3, 0.7, 0.5, 0, 0, 0, 0, 0) # Including intercept!


n_dirty <- round(dirty * n, 0)
n_clean <- n - n_dirty

# Design matrix
X_matrix <- matrix(nrow = n_clean, ncol = p)
X_matrix[, 1] <- rep(1, times = n_clean)
for(j in 2:p){
  X_matrix[, j] <- rnorm(n = n_clean)
}

# Linear predictor XB (eta)
eta_vector <- X_matrix %*% beta_vector

## Generating outcome
# Expected value (p sometimes pi)
prob_vector <- exp(eta_vector)/(1 + exp(eta_vector))

# Actual 0/1 Bernouilli outcomes
y_vector <- rbinom(n = n_clean,
                   size = 1,
                   prob = prob_vector)


# Dirty samples
X_dirty <- matrix(nrow = n_dirty, ncol = p)
X_dirty[, 1] <- rep(1, times = n_dirty)
for(j in 2:p){
  X_dirty[, j] <- runif(n = n_dirty, min = 0.7, max = 3)
}
y_dirty <- rep(0, times = n_dirty)

# Combining (adding the rows)
y <- c(y_vector, y_dirty)
X <- rbind(X_matrix, X_dirty)


# In data frame format
data_X       <- as.data.frame(cbind(X[, -1])) # Removing intercept because formula interface will do this automatically
names(data_X) <- paste0("X", 1:(p - 1))

X <- data.matrix(data_X)

table(y)
str(y)
```

__WE HAVE TO USE DATA.MATRIX, which is kind of strange__

```{r enetLTS_fitting}
enetLTS1 <- enetLTS(xx = data_temp,
                    yy = y,
                    family = "binomial",
                    intercept = TRUE, # Default: TRUE
                    nfold = 5, # Default: 5
                    repl = 3, # Default: 5
                    alpha = c(seq(0, 1, 0.05)),
                    lambdas = c(2e-5, 2e-4, 2e-3, 2e-2, 2e-1, 2e0, 3, 4, 5)) 
enetLTS1

enetLTS2 <- enetLTS(xx = X,
                    yy = y,
                    family = "binomial",
                    intercept = TRUE, # Default: TRUE
                    nfold = 2, # Default: 5
                    repl = 2, # Default: 5
                    alpha = seq(0, 1, length = 20),
                    lambdas = seq(0, 4, length =  40))
enetLTS2
summary(enetLTS2)
coef(enetLTS2)
-mean(y * log(unname(unlist(predict(enetLTS2, newX = X_test)))) + (1 - y) * log((1 - unname(unlist(predict(enetLTS2, newX = X_test))))))
# 0.7753325 # test loss on clean test data

enetLTS_test <- enetLTS(xx = X,
                    yy = y,
                    family = "binomial",
                    intercept = TRUE, # Default: TRUE
                    nfold = 2, # Default: 5
                    repl = 2, # Default: 5
                    alpha = seq(0, 1, length = 20),
                    lambdas = seq(0, 2, length =  40))


cvglmnet1 <- cv.glmnet(x = X,
                       y = y,
                       nfolds = 5,
                       family = "binomial") # Necessary here
coef(cvglmnet1, s = "lambda.1se")
coef(cvglmnet1, s = "lambda.min")
plot(cvglmnet1)

error_min <- cvglmnet1$cvm[cvglmnet1$lambda == cvglmnet1$lambda.min]
error_min


# Lambda min:
- mean(y * log(predict(cvglmnet1, newx = X_test, s = "lambda.min", type = "response")) + (1 - y) * log((1 - predict(cvglmnet1, newx = X_test, s = "lambda.min", type = "response"))))


- mean(y * log(predict(cvglmnet1, newx = X_test, s = "lambda.1se", type = "response")) + (1 - y) * log((1 - predict(cvglmnet1, newx = X_test, s = "lambda.1se", type = "response"))))

# MFC Classifier
table(y) # 0: 413 ; 1: 587
mfc_preds <- rep(1, n)

# The cross entropy loss is ofcourse not obtainable here, let's look at missclassification loss:
mean(y != mfc_preds)


# LASSO
lasso1 <- glmnet(x = X,
                 y = y,
                 family = "binomial",
                 alpha = 1)
coef(lasso1)


enetLTS3 <- enetLTS(xx = data_temp,
                    yy = y,
                    family = "binomial",
                    intercept = TRUE, # Default: TRUE
                    nfold = 2, # Default: 5
                    repl = 2)

enetLTS3



```

So even for 42 alpha and 80 lambda values, the grid was able to be displayed, while using the built-in option was not able to. So probably there is something happening in the logic internally.



Generation method according to paper (KHF 2017)
```{r}
#set.seed(123)

# Setting parameters
n <- 150 # Sample size
p <- 50 # Dimensionality (excluding intercept)
p_a <- 0.1 * p # Informative dimensionality (true dimensionality)
p_b <- p - p_a # Uninformative dimensionality

## Creating informative predictors
# Covariance matrix (sigma_a)
rho_a <- 0.9 # Base rho

sigma_a <- matrix(NA, nrow = p_a, ncol = p_a)
for(i in 1:p_a){
  for(j in 1:p_a){
    sigma_a[i, j] <- rho_a^(abs(i - j))
  }
}

X_a <- rmvnorm(n = n,
               mean = rep(0, p_a),
               sigma = sigma_a)

## Creating uninformative predictors
rho_b <- 0.5

sigma_b <- matrix(NA, nrow = p_b, ncol = p_b)

for(i in 1:p_b){
  for(j in 1:p_b){
    sigma_b[i, j] <- rho_b^(abs(i - j))
  }
}

X_b <- rmvnorm(n = n,
               mean = rep(0, p_b),
               sigma = sigma_b)

# Combining
X <- cbind(X_a, X_b)

# True Beta vector
beta <- c(rep(1, p_a), rep(0, p_b))

# Errors
e <- rnorm(n = n)

# Continuous Underlying DGP
y_cont <- 1 + X %*% beta + e

# Dichotomizing
y <- ifelse(test = y_cont > 0, yes = 1, no = 0)

## CONTAMINATION
# Leverage points
n_0 <- sum(y == 0)
n_1 <- sum(y == 1)

X_a[y == 0, ][1:floor(0.1*n_0), ] <- rmvnorm(n = floor(0.1 * n_0), mean = rep(20, p_a))
# Explanation: taking informative variables only, taking only observations equal to 0, and taking 10% of those and replacing by high values

y[y == 0][1:floor(0.1*n_0)] <- 1 # Wrong membership
# Same observations and assigning wrong class membership

# Combining again
X <- cbind(X_a, X_b)


## TEST DATA (NO OUTLIERS!)
X_test_a <- rmvnorm(n = n,
               mean = rep(0, p_a),
               sigma = sigma_a)
X_test_b <- rmvnorm(n = n,
               mean = rep(0, p_b),
               sigma = sigma_b)
X_test <- cbind(X_test_a, X_test_b)
e_test <- rnorm(n = n)
y_test_cont <- 1 + X_test %*% beta + e_test
y_test <- ifelse(test = y_test_cont > 0, yes = 1, no = 0)




```
Does this make sense, first we are making leverage points, then we actually fix the class memberships it seems...


Trying out some models using this DGP method:
```{r}
temp <- enetLTS(xx = X,
                yy = y,
                family = "binomial",
                nfold = 5,
                repl = 2)
temp

preds_temp <- unlist(unname(predict(temp, newX = X_test, vers = "reweighted", type = "response")))
preds_temp_class <- unlist(unname(predict(temp, newX = X_test, vers = "reweighted", type = "class")))

loss_temp <- - mean(y_test * log(preds_temp) + (1 - y_test) * log(1 - preds_temp))
loss_temp # 1.521147

misclass_loss_temp <- mean(preds_temp_class  != y_test)
misclass_loss_temp # 0.48

temp2 <- enetLTS(xx = X,
                 yy = y,
                 family = "binomial",
                 nfold = 5,
                 repl = 5,
                 lambdas = c(2e-7, 2e-6, 2e-5, 2e-4, 2e-3, 3e-2, 2e-2, 1e-2, 2e-1, 2e0, 2e1),
                 alphas = seq(0, 1, 0.05))

cv_lasso1 <- cv.glmnet(x = X,
                       y = y,
                       family = "binomial")
coef(cv_lasso1, s = "lambda.min")

cv_lasso1_preds <- predict(cv_lasso1, newx = X_test, s = "lambda.min", type = "response")
cv_lasso1_preds_class <- predict(cv_lasso1, newx = X_test, s = "lambda.min", type = "class")

loss_cv_lasso1 <- - mean(y_test * log(cv_lasso1_preds) + (1 - y_test) * log(1 - cv_lasso1_preds))
loss_cv_lasso1

misclass_loss_cv_lasso1 <- mean(cv_lasso1_preds_class != y_test)
misclass_loss_cv_lasso1

```


```{r}
# Generating data
train1 <- binary_reg_dgp1(100, 1, 1, beta = 1, dirty = 0.1, v_outlier = 0) # My idea of vertical outliers
train2 <- binary_reg_dgp1(100, 1, 1, beta = 1, dirty = 0.1, v_outlier = 1) # KHF idea of vertical outliers

## Fitting logits (ordinary)
logit1 <- glm(y ~ X, data = train1, family = "binomial")
logit2 <- glm(y ~ X, data = train2, family = "binomial")

coef(logit1)
coef(logit2)

# Checking loss
-logLik(logit1)/nrow(train1) # 0.602
-logLik(logit2)/nrow(train2) # 0.364 (BEST, minimizing risk/loss) -> Hence probably not real outliers

## Plotting logistic fit
# Generating predicted probabilities along a fine grid
x_grid <- seq(-100, 100, by = 0.01)

# Getting predictions
logit1_preds <- predict(logit1, 
                        newdata = data.frame(X = x_grid),
                        type = "response")
logit2_preds <- predict(logit2,
                        newdata = data.frame(X = x_grid),
                        type = "response")

# Plotting (Zoomed-out)
xlim <- c(-80, 80)
par(mfrow = c(1, 2))
plot(train1$X, train1$y, 
     xlim = xlim,
     main = "My (VW) outliers (NONrobust fit)", 
     xlab = "X",
     ylab = "Probability")
lines(x = x_grid, y = logit1_preds)

plot(train2$X, train2$y, 
     xlim = xlim,
     main = "KHF Outliers (NONrobust fit)", 
     xlab = "X",
     ylab = "Probability")
lines(x = x_grid, y = logit2_preds)
par(mfrow = c(1, 1))

# Plotting (Zoomed-in)
xlim <- c(-25, 25)
par(mfrow = c(1, 2))
plot(train1$X, train1$y,
     xlim = xlim,
     main = "My (VW) outliers (NONrobust fit)", 
     xlab = "X",
     ylab = "Probability")
lines(x = x_grid, y = logit1_preds)

plot(train2$X, train2$y, 
     xlim = xlim,
     main = "KHF Outliers (NONrobust fit)", 
     xlab = "X",
     ylab = "Probability")
lines(x = x_grid, y = logit2_preds)
par(mfrow = c(1, 1))
 

## Trying glmRob
logit_rob1 <- glmrob(y ~ X, data = train1, family = "binomial", method = "WBY")
logit_rob2 <- glmrob(y ~ X, data = train2, family = "binomial", method = "WBY")

coef(logit_rob1)
coef(logit_rob2)

# Predictions
# These don't want to work for some reason:
logit_rob1_preds <- predict(logit_rob1, newdata = data.frame(X = x_grid), type = "response")
logit_rob2_preds <- predict(logit_rob2, newdata = data.frame(X = x_grid), type = "response")

# MANUALLY getting predictions
logit_rob1_preds <- exp(coef(logit_rob1)[1] + coef(logit_rob1)[2] * x_grid)/(1 + exp(coef(logit_rob1)[1] + coef(logit_rob1)[2] * x_grid))
plot(x_grid, logit_rob1_preds)

logit_rob2_preds <- exp(coef(logit_rob2)[1] + coef(logit_rob2)[2] * x_grid)/(1 + exp(coef(logit_rob2)[1] + coef(logit_rob2)[2] * x_grid))
plot(x_grid, logit_rob2_preds)

# Plotting (Robust, Zoomed-out)
xlim <- c(-80, 80)
par(mfrow = c(1, 2))
plot(x = train1$X, 
     y = train1$y,
     xlim = xlim,
     main = "My (VW) outliers (ROBUST)", 
     xlab = "X",
     ylab = "Probability")
lines(x = x_grid, 
      y = logit_rob1_preds)

plot(x = train2$X, 
     y = train2$y,
     xlim = xlim,
     main = "KHF Outliers (ROBUST)",
     xlab = "X",
     ylab = "Probability")
lines(x = x_grid, 
      y = logit_rob2_preds)
par(mfrow = c(1, 1))

# Plotting (Robust, Zoomed-in)
xlim <- c(-25, 25)
par(mfrow = c(1, 2))
plot(x = train1$X, 
     y = train1$y, 
     xlim = xlim,
     main = "My (VW) outliers (ROBUST)", 
     xlab = "X",
     ylab = "Probability")
lines(x = x_grid, 
      y = logit_rob1_preds)

plot(x = train2$X, 
     y = train2$y, 
     xlim = xlim,
     main = "KHF Outliers (ROBUST)", 
     xlab = "X",
     ylab = "Probability")
lines(x = x_grid, 
      y = logit_rob2_preds)
par(mfrow = c(1, 1))
```


Let is use this setting to do a little simulation study:
```{r}
# Clean
reps <- 100
data <- vector("list", length = reps)
test <- vector("list", length = reps)
models <- vector("list", length = reps)
predictions <- matrix(NA, nrow = reps, ncol = 100)
loss <- vector("numeric", length = reps)
for(i in 1:reps){
  data[[i]] <- binary_reg_dgp1(100, 1, 1, beta = 1, dirty = 0, v_outlier = 0) # My idea of vertical outliers
  test[[i]] <- binary_reg_dgp1(100, 1, 1, beta = 1, dirty = 0, v_outlier = 0)
  data_now <- data[[i]]
  test_now <- test[[i]]
  y_test_now <- test_now[, 1]
  X_test_now <- test_now[, -1, drop = FALSE] # otherwise it makes it a vector :(
  
  models[[i]] <- glm(y ~ X, data = data_now, family = "binomial")
  predictions[i, ] <- predict(models[[i]], newdata = X_test_now, type = "response")
  loss[i] <- logit_loss(y_true = y_test_now, prob_hat = predictions[i, ])
  
}

coefs_clean <- t(sapply(models, coef))
mean_loss <- mean(loss)
 

# Nonrobust
reps <- 100
data <- vector("list", length = reps)
models <- vector("list", length = reps)
for(i in 1:reps){
  data[[i]] <- binary_reg_dgp1(100, 1, 1, beta = 1, dirty = 0.1, v_outlier = 0) # My idea of vertical outliers
  data_now <- data[[i]]
  models[[i]] <- glm(y ~ X, data = data_now, family = "binomial")
}

# Getting coefs of all models
coefs <- t(sapply(models, coef))


# ROBUST
reps <- 100
data <- vector("list", length = reps)
models <- vector("list", length = reps)
for(i in 1:reps){
  data[[i]] <- binary_reg_dgp1(100, 1, 1, beta = 1, dirty = 0.1, v_outlier = 0) # My idea of vertical outliers
  data_now <- data[[i]]
  models[[i]] <- glmrob(y ~ X, data = data_now, family = "binomial", method =  "WBY")
  # Needs (W)BY otherwise it screws up
}

coefs_rob <- t(sapply(models, coef))

# Plotting
par(mfrow = c(1, 3))
boxplot(x = coefs_clean,
        ylim = c(-3, 3),
        main = "glm fits CLEAN")
boxplot(x = coefs, 
        ylim = c(-3, 3),
        main = "glm fits DIRTY")
boxplot(x = coefs_rob, 
        ylim = c(-3, 3),
        main = "glmrob WBY DIRTY")

## Predictions
# Clean
clean_preds <- predict()

```

Hence it can be seen, the coefs for dirty glm() are completely off, glmrob WBY is doing well in comparison with clean glm!

High dimensional case
```{r}
reps <- 100
data <- vector("list", length = reps)
models <- vector("list", length = reps)

# Trying 1 run:
temp <- binary_reg_dgp1(n = 100, p = 60, p_a = 6, beta = c(rep(1, 6), rep(0, 54)), dirty = 0.05, v_outlier = 0)
temp_test <- binary_reg_dgp1(n = 100, p = 60, p_a = 6, beta = c(rep(1, 6), rep(0, 54)), dirty = 0, v_outlier = 0)

temp_y_test <- temp_test[, 1]
temp_X_test <- as.matrix(temp_test[, -1]) # as.matrix!

temp_y <- temp[, 1]
temp_X <- as.matrix(temp[, -1])

temp_enetlts1 <- enetLTS(xx = temp_X,
                         yy = temp_y,
                         family = "binomial",
                         nfold = 5,
                         repl = 5)
temp_enetlts1_preds <- unname(unlist(predict(temp_enetlts1, newX = temp_X_test, type = "response")))
logit_loss(y_true = temp_y_test, prob_hat = temp_enetlts1_preds)


temp_glmrob1 <- glmrob(y ~ ., data = temp, family = "binomial", method = "BY")

temp_glmrob1_preds <- predict(temp_glmrob1, type = "response")
logit_loss(y_true = temp_y, prob_hat = temp_glmrob1_preds)



for(i in 1:reps){
  data[[i]] <- binary_reg_dgp1(n = 100, p = 60, p_a = 6, beta = c(rep(1, 6), rep(0, 54)), dirty = 0.1, v_outlier = 0) # My idea of vertical outliers
  data_now <- data[[i]]
  models[[i]] <- enetLTS(y ~ X, data = data_now, family = "binomial", method =  "BY")
  # Needs WBY otherwise it screws up
}

temp_y * log(temp_enetlts1_preds) + (1 - temp_y) * log(1 - temp_enetlts1_preds)
```
Ofcourse logit loss on a test set WITH OUTLIERS is very high! because in that case loss would be infinity!!!!!!!

Alternate outlier creation:
```{r}
## Adjusting the outlier creations

# Setting parameters    
n <- 1000
dirty <- 0.03
p <- 10 # Including intercept
beta_vector <- c(1, 2, 3, 0.7, 0.5, 0, 0, 0, 0, 0) # Including intercept!
length(beta_vector)

# Derive the dirty amount of observations
n_dirty <- ceiling(n * dirty)


## Design matrix (Including intercept)
# Simple way
X_matrix <- matrix(nrow = n, ncol = p)
X_matrix[, 1] <- rep(1, times = n)
for(j in 2:p){
  X_matrix[, j] <- rnorm(n = n)
}

## Another way
#X_matrix <- matrix(nrow = n, ncol = p)
#X_matrix[, 1] <- rep(1, times = n)
#X_matrix[, -1] <- rmvnorm(n = n,mean = rep(0, p-1))

# Linear predictor XB (eta)
eta_vector <- X_matrix %*% beta_vector

## Generating outcome
# Expected value (p, sometimes denoted pi)
prob_vector <- exp(eta_vector)/(1 + exp(eta_vector))

# Actual 0/1 Bernouilli outcomes
y_vector <- rbinom(n = n,
                   size = 1,
                   prob = prob_vector)
y <- y_vector

## Inserting dirty samples
# Defining dirty predictors
dirty_predictors <- c(2, 3, 7, 8)


#X_matrix[y_vector == 0, dirty_predictors][1:n_dirty, dirty_predictors] <- X_matrix[1:n_dirty, dirty_predictors][1:n_dirty, dirty_predictors] + rnorm(n = 1, mean = 5)
X_matrix[y_vector == 0, dirty_predictors][1:n_dirty, ] <- X_matrix[1:n_dirty, dirty_predictors][1:n_dirty, ] + rnorm(n = 1, mean = 5)



data_X <- as.data.frame(X_matrix[, -1]) # Removing intercept because formula interface will do this automatically
names(data_X) <- paste0("X", 1:(p - 1))
X <- data.matrix(data_X)

table(y)
str(y)

cor(X)
```
