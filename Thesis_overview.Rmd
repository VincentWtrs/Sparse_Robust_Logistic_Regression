---
title: "Thesis_Overview"
author: "Vincent Wauters"
date: "27 February 2018"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r installing_packages}
#install.packages("enetLTS")
#install.packages("robustbase")
#install.packages("mvtnorm")
#install.packages("MASS") # confint
#install.packages("robust")
#install.packages("MatrixModels")
#install.packages("glmnet")
```

```{r loading_libraries}
library(enetLTS)
library(robustbase)
library(mvtnorm)
library(MASS)
library(robust)
library(MatrixModels)
library(glmnet)
```

Sourcing functions defined in separate .R files.
```{r sourcing_functions}
source(file = "C://Users/Vincent Wauters/Google Drive/Schoolwerk/KU Leuven/Thesis_Sparse_Robust_Logistic_Regression/binary_reg_dgp1.R")
source(file = "C://Users/Vincent Wauters/Google Drive/Schoolwerk/KU Leuven/Thesis_Sparse_Robust_Logistic_Regression/logit_sim.R")
source(file = "C://Users/Vincent Wauters/Google Drive/Schoolwerk/KU Leuven/Thesis_Sparse_Robust_Logistic_Regression/logit_loss.R")
```

To get a better grasp of the concepts of robust and sparse modelling some simple examples will be given that isolate single issues and apply it on a simple models.

# Linear Models
## Linear - Low Dimensional (LD)
### Linear - LD - No Outliers

__USING OLS__

In a first step data is generated without outliers. A simple data generating process (DGP) $y = \beta_0 + \beta_1 * x_1 + \varepsilon$ and the same model is estimated by OLS.
```{r data_and_ols_no_outliers}
## Creating data
# Setting sample size
n <- 500

# Setting Coviates (Simple fixed design)
x1 <- c(rep(x = 10, times = floor(n/2)), rep(x = 20, times = floor(n/2)))

# Setting beta0, beta1
beta0 <- 100
beta1 <- 10

# Defining error
e <- rnorm(n = n,
            mean = 0,
            sd = 5)

# Defining the outcome
y <- beta0 + beta1 * x1 + e

## Fitting OLS
lm1 <- lm(y ~ x1) # (Includes intercept)
summary(lm1)

## Plotting
plot(x = x1,
     y = y,
     main = "No outliers, OLS fit")
abline(lm1)
```

__USING LTS__

A robust estimator for the linear model, the __Least Trimmed Squares (LTS)__ estimator can also be used, and should give the same results. The function `ltsReg()` (capital R) from the `robustbase` can be used. The default $\alpha = 0.5$ this leads to a $h$ sample size of $h = $ __to do__
```{r LTS_linear_no_outliers}
# Fitting LTS
lts1 <- ltsReg(y ~ x1, alpha = 0.5) # Default
summary(lts1)

# Fitting LTS with less trimming
lts2 <- ltsReg(y ~ x1, alpha = 0.95) # Closer to OLS
summary(lts2)

## Plotting LTS
plot(x = x1,
     y = y,
     main = "Clean data: OLS vs. LTS")
abline(lts1, col = "black") # LTS Line
abline(lm1, col = "gray", lwd = 2, lty = 2)
legend("bottomright",
       legend = c("OLS", "LTS"),
       lty = 1:2,
       lwd = 1:2,
       col = c("black", "gray"))
```

__CONCLUSION: Yes in the linear low dimensional case, LTS seem to do equally good (visually at least) as OLS in clean case__

__TO DO: what's the relation between alpha and the error variance (scale) estimate?__
__TO DO: I have no idea why the degrees of freedom on the residuals is not $n - p?!__


### Linear - LD - Outliers
Let's now introduce some __leverage points__, these are points that are outlying in the X direction. There are __good leverage points__ which have outlying X values but have a y-value that corresponds to the value that would be taken on by the relation follows from the other points. __Bad leverage points__ have outlying X values and have a non-fitting y value. 

Bad leverage points will be introduced, set at the coordinate (30, 150 + $e$) with $e$ coming from a $N(0, \sigma = 5)$ distribution or just put otherwise $y \sim N(150, 5)$. This is certainly a bad leverage point because a predicted reasonable value for $x1 = 30$ would be around 400.
```{r bad_leverage_point}
# Take look at current dataset
plot(x = x1,
     y = y,
     main = "Outliers Visualization in Linear Models")
abline(a = coef(lm1)[1],
       b = coef(lm1)[2])

# Introducing leverage points
n_outlier1 <- 50 # Amount of outliers
n2 <- n1 + n_outlier1

# Making them bad outliers
outliers1 <- rnorm(mean = 150, sd = 5, n = n_outlier1)

x1 <- c(x1, rep(30, n_outlier1))
y <- c(y, outliers1)

# Plotting
plot(x = x1,
     y = y,
     main = "OLS fit on regular data, bad leverage points added to display")
abline(lm1)
```

When fitting a model to this model that has 50/550 leverage points using OLS but also the LTS estimator:
```{r bad_leverage_ols_lts}
# Fitting OLS
lm2 <- lm(y ~ x1)
summary(lm2)

# Fitting LTS
lts2 <- ltsReg(y ~ x1)
summary(lts2)

# Plotting
plot(x = x1, 
     y = y,
     main = "Impact of bad leverage points")
abline(lm1, col = "gray", lwd = 2) # OLS on regular data only
abline(lm2, col = "black") # OLS on all data
abline(lts2, col = "red") # LTS on all dat
legend("bottomleft",
       legend = c("OLS no outliers",
                  "OLS outliers",
                  "LTS outliers"),
       lty = 1,
       col = c("gray", "black", "red"))
# OLS on the non-outlying points versus LTS: same, good!

```

Let's take a quick look at the LTS regression results on this dirty data:
```{r bad_leverage_lts_detail}
summary(lts2)
coef(lts2)

# Real betas:
beta0; beta1

```

The LTS recovers the true parameter values.

## Linear - High Dimensional
### Linear - HD - No Outliers

Let's now make something that is a lot more high dimensional, something with p = 200 (with intercept: 201), but only 10 first betas are nonzero, and the sample size stays the same at 500, __this example is thus NOT n < p!__

```{r}
## Setting parameters
n1 # Keepign same sample size = 500
p_a <- 10 # True dimensionality
p_b <- 190 # Uninformative
p <- p_a + p_b # Total dimensionality

## Creating data
beta0 <- 1
beta <- c(beta0, rep(1, p_a), rep(0, p_b))

# Creating covariates
X <- rmvnorm(n = n1,
             mean = rep(0, p),
             sigma = diag(p))
# Adding intercept column
X <- cbind(rep(1, times = n1), X)

# Adding names
colnames(X) <- paste0("X", 0:p)
  
# Creating outcome
y <- X %*% beta + rmvnorm(n = n1,
                          mean = 0)

# Creating dataframe used for FITTING
data1 <- data.frame(cbind(y, X[, -1]))
names(data1)[1] <- "y" # Setting name of y (first col)

## Fitting
# Making formula
formula1 <- paste("y", paste0("X", 1:p, collapse = " + "), sep = " ~ ")

# OLS
lm3 <- lm(formula = formula1, data = data1)
summary(lm3)

# LTS
lts3 <- ltsReg(formula = formula1, data = data1)
```
__ TO DO: further check the lts3 problems__

# Logistic Regression
## Logistic - Low Dimensional
### Logistic - LD - No Outliers

If we want multiple runs of such model (i.e. simulation), we need to run it multiple times, now we will automate this using a very simple function (which we will not use much further on, I suppose) and then using `replicate()` on this function. The function will generate data and specify the DGP and fit a logit model, then get the coefficients. By replicating this many times, due to the slightly different data (X) every time being sampled __and__ also due to sampling from the binomial distribution (i.e. even with the exact same probability parameter we ofcourse get different realizations).

The function is very basic, the parameters cannot even be altered in this simple version, it's purely as an example now!
```{r temporary_logitcoefs_simulation_function}
## Very Simple logit simulation: coefficients
logitcoefs <- function() {
  # Sample size
  N <- 1000
  
  # Beta
  beta0 <- 1
  beta1 <- 2
  beta2 <- 3
  
  # Linear predictor (eta)
  x1 <- rnorm(N)
  x2 <- rnorm(N)
  eta <- beta0 + beta1 * x1 + beta2 * x2
  
  # Probability (pi or p) and outcome (0/1)
  p <- exp(eta)/(1 + exp(eta))
  y <- rbinom(n = N, size = 1, prob = p)
  
  # Data.frame
  df <- data.frame(y = y, x1 = x1, x2 = x2)
  
  # Fitting
  fit <- glm(y ~ x1 + x2, data = df, family = binomial(link = "logit"))
  
  # Returning coefs
  return(coef(fit))
}

# Replicating and getting results:
results <- t(replicate(n = 50, 
                       expr = logitcoefs())) # Transposing for more easy to read results
colMeans(results) # Averages per variable
# Comparing to original beta = 1, 2, 3 -> YES It's OK

```


__to do: recheck these:__
Running simulations: first set up all the wanted settings, then looping over all these settings. To facilitate this we will create a settings matrix called `settings`.
```{r}
# Defining all simulation settings
beta_vector <- c(1, 2, 3) # True betas, same for all
n <- 500 # Sample size
runs <- 199
dirty <- 0
#types <- c("glm", "glmrob")
types <- "enetLTS"

# Putting in matrix
settings <- expand.grid(n = n, runs = runs, dirty = dirty, type = types)
settings
```

And each time gather all the settings by keeping the names the same every time and assesing row by row. Thus we are looping over the settings:
```{r simulation_loop_over_settings}
# Making model list of length = amount of rows of settings
model_list <- vector("list", nrow(settings))

# Looping over the settings:
for(i in 1:nrow(settings)){
  
  # To get an idea of progression
  print(paste("iteration", i))
  
  # Extracting current row of settings
  current_settings <- settings[i, ]
  
  # Saving all seperate settings in different variables
  n <- current_settings$n
  runs <- current_settings$runs
  dirty <- current_settings$dirty
  type <- current_settings$type

  # Running models
  model_list[[i]] <- logit_sim(beta_vector = beta_vector,
                               n = n,
                               runs = runs,
                               dirty = dirty,
                               type = type)
  
}
```

__to do: maybe add somekind of fields that contain the simulation settings__

Let us quickly define a simulation summary function that returns:

* The true coefficients, as set by the user
* The estimated means of the coefficients (mean over simulations)
* 
```{r sim_summary_function}
# Summary of simulation function:
sim_summary <- function(sim_object, beta_vector){
  coefs <- t(sapply(sim_object, coef))
  true <- beta_vector
  mean <- colMeans(coefs)
  bias <- colMeans(coefs - beta_vector)
  mse <- colMeans((coefs - beta_vector)^2)
  
  results <- rbind(true, mean, bias, mse)
  return(results)
}
```

And check the results
```{r}
lapply(model_list, sim_summary, beta_vector)
```


### Logistic - LD - With Outliers
Let's try a very trivial __clean data__ setup, which does not actually allows me directly to see what the true $\beta$ is but gives some insights, far from ideal. We will take an increasing predictor $x_i = 1, 2, 3, ..., 100$ and an associated outcome $y_i = 0, ... 0, 1, ..., 1$, i.e. 0 for the first $2n/5$ (40%) and 1 for the last $2n/5$ (40%) observations, __for the middle we need some overlap in the observations to not have perfect separation__. To do this the middle $n/5$ (20%) variables are generated according to a Bernouilli distribution with $p = 0.5$.

```{r logit_manual_ld}
# Setting amount of obs:
n <- 200

# Predictors
X <- 1:n
  
# Outcome
set.seed(1234)
y <- c(rep(0, ((2/5) * n)), # 40 pct 0, 40pct 1, 20 pct: rbinom 50/50 in the middle to get OVERLAP
       rbinom(n = n/5, size = 1, prob = 0.50),
       rep(1, (2/5) * n))

# Data.frame:
data <- data.frame(y = y,
                   X = X)

# Plotting
plot(x = X,
     y = y,
     main = "Visualizing binary data")
```

Only clean data:
```{r plotting_logit}
# Fitting logit
logit1 <- glm(y ~ X, 
              family = binomial(link = "logit"),
              data = data)
logit1

# Getting predictions to plot:
logit1_preds_prob <- predict(logit1, type = "response")
logit1_preds_class <- ifelse(test = logit1_preds_prob > 0.5, yes = 1, no = 0)

# Plotting
plot(x = X,
     y = y,
     main = "Unidimensional Logit - No Outliers")
abline(h = 0.5, lty = 2)
lines(x = X,
      y = logit1_preds_prob)
points(x = X, 
       y = logit1_preds_class,
       pch = 2,
       col = "red")
legend("bottomright",
       legend = c("True Outcome",
                  "Class Prediction"),
       pch = 1:2,
       col = 1:2)
```

Now let's add leverage points to the example above somehow. We see that for X-values above (ca.) 100 the predictions are all 1, while those for X-values below 100 are 0. So let's try to add some 0-outcomes for those X-values above 200 (max of range of clean data), this will be very atypical, a leverage point. We will do this for the last 10 points of data (X = 190, ..., X = 200)
```{r plotting_logit_leverage}
# Setting y = 0 where it should be 1
data_dirty <- data # Copy
data_dirty[190:200, 1] <- 0 # Contaminate
data_dirty[190:200, 2] <- 290:300 # More leverage


# Fitting on dirty data
logit2 <- glm(y ~ X, 
              family = binomial(link = "logit"),
              data = data_dirty)
summary(logit2)

# Predictions
logit2_preds_prob <- predict(logit2, type = "response")
logit2_preds_class <- ifelse(test = logit2_preds_prob > 0.5, yes = 1, no = 0)

# Plotting
plot(x = data_dirty$X,
     y = data_dirty$y,
     main = "Unidimensional Logit - WITH Leverage Points",
     xlab = "X",
     ylab = "y")
lines(x = data_dirty$X,
      y = logit2_preds_prob)
points(x = X,
       y = logit2_preds_class,
       pch = 2,
       col = "red")
legend("topleft",
       legend = c("True Outcome",
                  "Class Prediction"),
       pch = 1:2,
       col = 1:2)

# Comparing coefs clean vs. dirty
coef(logit1)
coef(logit2)
```

Seeing how leverage points influence the fitted logit model and the related coefficients, the impact is obviously huge. However we actually not really know what the true value of the betas should be here due to our very simplistic setup!

Hence let's assume the following setup: we assume a Bernouilli distributed outcome, parametrized by the probability parmaeter $p$, and this $p = f(X, \beta)$, i.e. is some function of a structural component parametrized by $X$ and $\beta$. (See also: https://stats.stackexchange.com/questions/127226/logistic-regression-simulation-in-order-to-show-that-intercept-is-biased-when-y)

First, let us remind ourselves of the __logistic__ transformation (function), which is not the __logit__ transformation (it is the inverse logit). Let's present the logistic function in two equivalent forms:
$$f(x) = \frac{e^x}{1+e^x} = \frac{1}{1 + e^{-x}}$$

While the logit function is the inverse of this function and is in very general notation equal to:
$$f(x) = log(\frac{x}{1 - x})$$

Also note we can write the odds as the exponential taken of the log odds, as it is a 0-operation:

$$odds = \exp(log(odds))$$
Then see one can write the probability of the event $p$ (sometimes denotes as $\pi$) as (note we now start going from $x$ to $p$ to get a bit less general and more into the logit framework/language)

$$p = \frac{odds}{1 + odds}$$
To see why, remember that $odds = \frac{p}{1-p}$ so rewriting the above statement leads to the following, they should be equivalent (i.e. $p = p$) if the above statement is true

$$p = \frac{p/(1-p)}{1 + p/(1-p)} $$
$$p = \frac{p/(1-p)}{[(1-p)/(1-p)] + [p/(1-p)]} = \frac{p/(1-p)}{(1 - p + p)/(1-p)} = \frac{p/(1-p)}{1/(1-p)} = \frac{p (1-p)}{1 - p} = p$$
So indeed we have established that $p = \frac{odds}{1 + odds}$

Why is this useful? Because we also know that

$$log(odds) = \boldsymbol{X \beta}$$

In other words: the logit model is linear in terms of log-odds.
```{r logit_ld_alternative_dgp}
# Defining log-odds to probability function
lo2p = function(lo){
  odds = exp(lo)        
  p    = odds / (1 + odds)
  return(p)
}

# Setting DGP parameters
n <- 1000  # 1000 obs
beta0 <- 0.25 # intercept
beta1 <- 1 # slope
r <- 200 # amount of simulations

# Generating data
X <- rnorm(n) # Standard normal covariate
lo <- beta0 + beta1 * X # log-odds
p <- lo2p(lo) # Parameter vector of the Bernouilli
y <- rbinom(n = n, size = 1, prob = p) # 0/1 outcomes parametrized by p

# Also Ordering the data according to increasing X, makes life easier
idX <- order(X) # Indices, yes do this in 2 steps, otherwise messy!

X <- X[idX]
y <- y[idX]

# Running a single logit
logit3 <- glm(y ~ X,
              family = binomial(link = "logit"))

# Getting predictions to plot:
logit3_preds_prob <- predict(logit3, type = "response")
logit3_preds_class <- ifelse(test = logit3_preds_prob > 0.5, yes = 1, no = 0)



## Plotting
# True points
plot(x = X,
     y = y)
# Logit line
lines(x = X,
      y = logit3_preds_prob)

# Predicted points
points(x = X,
       y = logit3_preds_class,
       pch = 2,
       col = "red")
# Legend
legend("bottomright",
       legend = c("True Outcome",
                  "Class Prediction"),
       pch = 1:2,
       col = 1:2)

# These will store the estimation results
beta_hat0 <- numeric(r)  
beta_hat1 <- numeric(r)
y1_prop <- numeric(r) # Proportion of y = 1 in generated samples

# Running simulation (note: only y is random now, X fixed over samples)
for(i in 1:r){
  y <- rbinom(n = n, size = 1, prob = p)
  model <- glm(y ~ X, family = binomial(link = "logit"))
  beta_hat0[i] <- coef(model)[1]
  beta_hat1[i] <- coef(model)[2]
  y1_prop[i] <- mean(y)
}

## Results (over all simulation runs)
# Beta0
mean(beta_hat0)  
sd(beta_hat0)

# Beta1
mean(beta_hat1)
sd(beta_hat1)

# Proportion y = 1
mean(y1_prop) # Slightly unbalanced at beta0 = 0.25, beta1 = 1
```

Let's add some outliers now to the alternate DGP we had before. Therefore we take the last e.g. 50 points of the ordered X-values, make them a tad bigger (e.g. 5) and make the corresponding y-values 0 instead of 1.
```{r logit_alternative_contamination}
## Single logit
# Contamination
X_dirty <- X
X_dirty[950:1000] <- 5 + rnorm(n = length(X_dirty[950:1000]), mean = 0, sd = 1)

y_dirty <- y
y_dirty[950:1000] <- 0

# Also Ordering the data according to increasing X, makes life easier
idX <- order(X_dirty) # Indices, yes do this in 2 steps, otherwise messy!

X_dirty <- X_dirty[idX]
y_dirty <- y_dirty[idX]

# Fitting
logit4 <- glm(y_dirty ~ X_dirty,
              family = binomial(link = "logit"))
coef(logit4) ; coef(logit3) # The contaminated one is completely off

# Predicting
logit4_preds_prob <- predict(logit4, type = "response")
logit4_preds_class <- ifelse(test = logit4_preds_prob > 0.5, yes = 1, no = 0)

## Plotting
# True points
plot(x = X_dirty,
     y = y_dirty,
     xlim = c(-3, 5.1),
     main = "Comparison of logit on clean data versus contaminated data")
# Logit line (Clean only)
lines(x = X,
      y = logit3_preds_prob,
      lty = 2,
      col = "gray")
# Logit line (contaminated data)
lines(x = X_dirty,
      y = logit4_preds_prob)
# Predicted Classes (On dirty)
points(x = X,
       y = logit4_preds_class,
       pch = 2,
       col = "red")
# Legend
legend("topright",
       legend = c("True Outcome",
                  "Class Prediction"),
       pch = 1:2,
       col = 1:2)
legend("topleft",
       legend = c("Logit: All data",
                  "Logit: Clean data only"),
       lty = 1:2,
       col = c("black", "gray"))

```

## Logistic - High Dimensional
### Logistic - HD - No Outliers

Now that we have the data generating process function and plotting is not doable anymore due to the multiple dimensions. Hence we will not split up the outlying/non-outlying cases in different titles. So we simply use the DGP functions from before, set the dimensionality to be higher, amount of contamination to 0 and run some fitting functions such as `glm()` and `glmnet()` and `enetLTS()`.

```{r}
# Creating training set (clean in this first case)
train1 <- binary_reg_dgp2(n = 100,
                          p = 50,
                          p_a = 5,
                          beta = c(rep(1, 5), rep(0, 45)),
                          dirty = 0,
                          type = "bernoulli")

# Creating a test set (ALWAYS clean data!)
test1 <- binary_reg_dgp2(n = 100,
                          p = 50,
                          p_a = 5,
                          beta = c(rep(1, 5), rep(0, 45)),
                          dirty = 0,
                          type = "bernoulli")
# Ordinary logit
logit5 <- glm(y ~ ., 
              family = binomial(link = "logit"),
              control = glm.control(maxit = 200),
              data = train1)

# Coefficients
coef(logit5)

# Predictions
logit5_preds_prob <- predict(logit5,
                             newx = test1_X,
                             type = "response")
logit5_loss <- logit_loss(y_true = test1_y,
                          prob_hat = logit5_preds_prob)

### GLMNET
# Converting data
train1_X <- data.matrix(train1)[, -1]
train1_y <- data.matrix(train1)[, 1]

test1_X <- data.matrix(test1)[, -1]
test1_y <- data.matrix(test1)[, 1]

## LASSO logit (glmnet: alpha = 1)
lasso_logit1 <- cv.glmnet(x = train1_X,
                          y = train1_y,
                          family = "binomial",
                          alpha = 1,
                          type.measure = "deviance")
# Coefficients
coef(lasso_logit1, s = "lambda.min") # Absolute minimum lambda attained over CV
coef(lasso_logit1, s = "lambda.1se") # 1SE rule (more spare models)

# Predictions
lasso_logit1_preds_prob <- predict(lasso_logit1, 
                                   newx = test1_X,
                                   type = "response",
                                   s = "lambda.min")
# Loss
lasso_logit1_loss <- logit_loss(y_true = test1_y,
           prob_hat = lasso_logit1_preds_prob)


## Ridge logit (glmnet: alpha = 0)
ridge_logit1 <- cv.glmnet(x = train1_X,
                          y = train1_y,
                          family = "binomial",
                          alpha = 0,
                          type.measure = "deviance")
# Coefficients
coef(ridge_logit1, s = "lambda.min")
coef(ridge_logit1, s = "lambda.1se")

# Predictions
ridge_logit1_preds_prob <- predict(ridge_logit1, 
                                   newx = test1_X,
                                   type = "response",
                                   s = "lambda.min")
# Loss
ridge_logit1_loss <- logit_loss(y_true = test1_y,
                    prob_hat = ridge_logit1_preds_prob)

## Elastic Net logit (e.g. alpha = 0.75)
enet_logit1 <- cv.glmnet(x = train1_X,
                         y = train1_y,
                         family = "binomial",
                         alpha = 0.75,
                         type.measure = "deviance")
# Coefficients
coef(enet_logit1, s = "lambda.min")
coef(enet_logit1, s = "lambda.1se")

# Predictions
enet_logit1_preds_prob <- predict(enet_logit1,
                                  newx = test1_X,
                                  type = "response",
                                  s = "lambda.min")
# Loss
enet_logit1_loss <- logit_loss(y_true = test1_y,
                               prob_hat = enet_logit1_preds_prob)

### enetLTS
enetLTS_logit1 <- enetLTS(xx = train1[, -1],
                    yy = train1[, 1],
                    family = "binomial")

# Predictions
enetLTS_logit1_preds_prob <- predict(enetLTS_logit1, 
                                     newX = test1_X,
                                     vers = "reweighted",
                                     type = "response")
enetLTS_logit1_preds_prob <- unlist(unname(enetLTS_logit1_preds_prob))

# Loss
enetLTS_logit1_loss <- logit_loss(y_true = test1_y,
                                  prob_hat = enetLTS_logit1_preds_prob)


## Plotting losses in boxplot
# Combining losses in matrix
loss_comparison1 <- cbind(logit5_loss$loss, ridge_logit1_loss$loss, lasso_logit1_loss$loss, enet_logit1_loss$loss, enetLTS_logit1_loss$loss)
colnames(loss_comparison1) <- c("logit", "lasso", "ridge", "a .75", "enetLTS") # Giving column names

# Plotting
boxplot(loss_comparison1, 
        main = "Comparison of average loss on some Elastic Net Models")

# Plotting (zoomed in)
boxplot(loss_comparison1,
        main = "Comparison of average loss on some Elastic Net Models",
        ylim = c(0, 1))

# Other view: checking means of losses
loss_mean_comparison2 <- cbind(logit5_loss$avg_loss, lasso_logit1_loss$avg_loss, ridge_logit1_loss$avg_loss, enet_logit1_loss$avg_loss, enetLTS_logit1_loss$avg_loss)
colnames(loss_mean_comparison2) <- c("logit", "lasso", "ridge", "a .75", "enetLTS")

loss_mean_comparison2
```

So we see the models are very comparable in terms of loss, but ofcourse no outliers are introduced yet here!

__TO DO: maybe cv.glmnet tuned for alpha as well (manually)__

### Logistic - HD - With Outliers 

We will first create a training set (with contaminated data) and a clean dataset (same DGP, but setting dirty = 0). Then we run enetLTS using the first column of the training set as the outcome (`yy`) and all the other columns as the covariates (`xx`).

```{r logit_HD_dirty_fitting}
# Creating a training set
train1_dirty <- binary_reg_dgp2(n = 100,
                                p = 50,
                                p_a = 5,
                                beta = c(rep(1, 5), rep(0, 45)),
                                dirty = 0.1,
                                type = "bernoulli")
train1_dirty_X <- data.matrix(train1_dirty[, -1])
train1_dirty_y <- data.matrix(train1_dirty[, 1])

# Test set (just take one from before: HAS TO BE CLEAN!!!)

## logit (ordinary)
logit6 <- glm(y ~ .,
              family = binomial(link = "logit"),
              control = glm.control(maxit = 200),
              data = train1_dirty)

# Predictions
logit6_preds_prob <- predict(logit6,
                             newdata = data.frame(test1_X),
                             type = "response")
# Loss
logit6_loss <- logit_loss(y_true = test1_y,
                          prob_hat = logit6_preds_prob)

### glmnet
## LASSO
lasso_logit2 <- cv.glmnet(x = train1_dirty_X,
                          y = train1_dirty_y,
                          family = "binomial",
                          alpha = 1,
                          type.measure = "deviance")

# Predictions
lasso_logit2_preds_prob <- predict(lasso_logit2,
                                   newx = test1_X,
                                   type = "response",
                                   s = "lambda.min")
# Loss
lasso_logit2_loss <- logit_loss(y_true = test1_y,
                                prob_hat = lasso_logit2_preds_prob)



## Ridge
ridge_logit2 <- cv.glmnet(x = train1_dirty_X,
                          y = train1_dirty_y,
                          family = "binomial",
                          alpha = 0,
                          type.measure = "deviance")

# Predictions
ridge_logit2_preds_prob <- predict(ridge_logit2,
                                   newx = test1_X,
                                   type = "response",
                                   s = "lambda.min")
# Loss
ridge_logit2_loss <- logit_loss(y_true = test1_y,
                                prob_hat = ridge_logit2_preds_prob)

## Elastic net: Alpha = 0.75
enet_logit2 <- cv.glmnet(x = train1_dirty_X,
                         y = train1_dirty_y,
                         family = "binomial",
                         alpha = 0.75,
                         type.measure = "deviance")

# Predictions
enet_logit2_preds_prob <- predict(enet_logit2,
                                  newx = test1_X,
                                  type = "response",
                                  s = "lambda.min")
# Loss
enet_logit2_loss <- logit_loss(y_true = test1_y,
                               prob_hat = enet_logit2_preds_prob)



## DEFAULT enetLTS
enetLTS_logit2 <- enetLTS(xx = train1_dirty[, -1],
                    yy = train1_dirty[, 1],
                    family = "binomial")

# Predictions
enetLTS_logit2_preds_prob <- unlist(unname(predict(enetLTS_logit2,
                                                   newX = test1_X,
                                                   type = "response",
                                                   version = "reweighted")))

# Loss
enetLTS_logit2_loss <- logit_loss(y_true = test1_y,
                                  prob_hat = enetLTS_logit2_preds_prob)

## Plotting losses
loss_comparison2 <- cbind(logit6_loss$loss, lasso_logit2_loss$loss, ridge_logit2_loss$loss, enet_logit2_loss$loss, enetLTS_logit2_loss$loss)
colnames(loss_comparison2) <- c("logit", "lasso", "ridge", "a .75", "enetLTS")

# Plot
boxplot(loss_comparison2)


# Plot zoomed-in
boxplot(loss_comparison2,
        ylim = c(0, 1))

# Comparison with clean data and dirty data together
loss_comparison3 <- cbind(loss_comparison1, loss_comparison2)
boxplot(loss_comparison3)
boxplot(loss_comparison3, ylim = c(0, 1))

# Not always a good view: checking means of loss
loss_mean_comparison3 <- cbind(logit6_loss$avg_loss, lasso_logit2_loss$avg_loss, ridge_logit2_loss$avg_loss, enet_logit2_loss$avg_loss, enetLTS_logit2_loss$avg_loss)
colnames(loss_mean_comparison3) <- c("logit", "lasso", "ridge", "a .75", "enetLTS")

loss_mean_comparison3



# enetLTS: User supplied alpha/lambdas
enetLTS2 <- enetLTS(xx = train1_dirty[, -1],
                    yy = train1_dirty[, 1],
                    family = "binomial",
                    intercept = TRUE, # Default: TRUE
                    nfold = 5, # Default: 5
                    repl = 3, # Default: 5
                    alpha = c(seq(0, 1, 0.05)),
                    lambdas = c(2e-5, 2e-4, 2e-3, 2e-2, 2e-1, 2e0, 3, 4, 5))
enetLTS2

# enetLTS3: User supplied amount of alpha/lambdas
enetLTS3 <- enetLTS(xx = train1_dirty[, -1],
                    yy = train1_dirty[, 1],
                    family = "binomial",
                    intercept = TRUE, # Default: TRUE
                    nfold = 2, # Default: 5
                    repl = 2, # Default: 5
                    alpha = seq(0, 1, length = 20),
                    lambdas = seq(0, 4, length =  40))
enetLTS3
```

What about the trivial Most Frequent Class (MFC) classifier. However, this classifier does not produce predicted probabilities, just classes, hence the cross entropy loss cannot be calculated. But the misclassification loss (0-1) loss can be calculated ofcourse.

__TO DO__
